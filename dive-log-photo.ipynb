{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82663238-3081-4ba3-9f71-8ce67f8db9ff",
   "metadata": {},
   "source": [
    "This cell loads all the important libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4b428-06a7-49ec-9b89-cb8baeefca0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from exif import Image\n",
    "from iptcinfo3 import IPTCInfo\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import requests\n",
    "import json\n",
    "import shutil\n",
    "import time\n",
    "import zipfile\n",
    "\n",
    "thumbsDir = 'thumbs'\n",
    "fullDir = 'full'\n",
    "logDir = 'logs'\n",
    "observationDir = 'observations'\n",
    "speciesDir = 'species'\n",
    "outputDir = \"output\"\n",
    "diveLog = ET.parse(logDir + '/' + 'log.uddf')\n",
    "ORCID = \"https://orcid.org/0000-0002-3639-2080\"\n",
    "root = diveLog.getroot()\n",
    "worms_endpoint = \"https://www.marinespecies.org/rest\"\n",
    "taxonHierarchy = [\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n",
    "\n",
    "def getWoRMSAphiaID(taxon):\n",
    "    import requests\n",
    "    import json\n",
    "    result = {}\n",
    "    aphiaID = \"\"\n",
    "    worms_endpoint = \"https://www.marinespecies.org/rest/\"\n",
    "    operation = \"AphiaIDByName/\"\n",
    "    url = worms_endpoint + operation + taxon + \"?marine_only=true\"\n",
    "    response = requests.request(\"GET\", url)\n",
    "    if ( response.status_code == 200 ):\n",
    "        aphiaID = response.text\n",
    "    elif ( response.status_code == 206 ):\n",
    "        operation = \"AphiaRecordsByName/\"\n",
    "        url = worms_endpoint + operation + taxon + \"?marine_only=true&like=false\"\n",
    "        response = requests.request(\"GET\", url)\n",
    "        if ( response.status_code == 200 ):\n",
    "            data = json.loads(response.text)\n",
    "            for record in data:\n",
    "                if (record[\"status\"] == \"accepted\"):\n",
    "                    aphiaID = record[\"AphiaID\"]\n",
    "    result[\"response_code\"] = response.status_code\n",
    "    result[\"AphiaID\"] = aphiaID\n",
    "    return aphiaID\n",
    "\n",
    "def getWoRMSTaxonData(taxon):\n",
    "    import requests\n",
    "    aphiaID = getWoRMSAphiaID(taxon)\n",
    "    worms_endpoint = \"https://www.marinespecies.org/rest/\"\n",
    "    operation = \"AphiaRecordByAphiaID/\"\n",
    "\n",
    "    url = worms_endpoint + operation + str(aphiaID) + \"?marine_only=true\"\n",
    "\n",
    "    response = requests.request(\"GET\", url)\n",
    "    return response.text\n",
    "\n",
    "def getEOLID(taxon):\n",
    "    import requests\n",
    "    import json\n",
    "    EOLID = 0\n",
    "    url = \"https://eol.org/api/search/1.0.json?page=1&key=&exact=true&q=\" + taxon\n",
    "    \n",
    "    eol_endpoint = \"https://eol.org/api/\"\n",
    "    operation = \"search/1.0.json?page=1&key=&exact=true&q=\"\n",
    "    url = eol_endpoint + operation + taxon\n",
    "\n",
    "    response = requests.request(\"GET\", url)\n",
    "    \n",
    "    if (response.status_code == 200):\n",
    "        data = json.loads(response.text)\n",
    "        if ( data[\"totalResults\"] > 0):\n",
    "            EOLID = data[\"results\"][0][\"id\"]\n",
    "    return EOLID\n",
    "\n",
    "def getAWikipediaURL( hierarchy ):\n",
    "    import time\n",
    "    import wikipediaapi\n",
    "    wiki = wikipediaapi.Wikipedia('en')\n",
    "    url = \"\"\n",
    "    for taxon in hierarchy:\n",
    "        page = wiki.page(taxon)\n",
    "        if ( page.exists() ):\n",
    "            url = page.fullurl\n",
    "            break\n",
    "        time.sleep(0.1)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a197f954-104a-4930-9bb1-c8d78b3e0a9f",
   "metadata": {},
   "source": [
    "Spit out photo metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2bc077-ec8b-4666-8f61-93995f204a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "with os.scandir(fullDir) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_file():\n",
    "            with open(fullDir + '/' + entry.name, 'rb') as photo:\n",
    "                thisPhotoEXIF = Image(photo)\n",
    "                thisPhotoIPTC = IPTCInfo(photo)\n",
    "                print(thisPhotoIPTC[\"object name\"])\n",
    "                occurrenceRemark = \"\"\n",
    "                try:\n",
    "                    occurrenceRemark = thisPhotoEXIF.get(\"image_description\")\n",
    "                except:\n",
    "                    print(\"Couldn't extract image description\")\n",
    "                print(occurrenceRemark)\n",
    "                photoDateTime = datetime.fromisoformat(thisPhotoEXIF.datetime_original.replace(\":\", \"-\", 2))\n",
    "                print(entry.name + ' was taken at ' + photoDateTime.isoformat() + \" by \" + thisPhotoEXIF.artist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75e2e97-dfa6-4c72-95a7-34c313a39cef",
   "metadata": {},
   "source": [
    "Grab latitude and longitude for a given siteID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e182467f-6385-4f23-99af-6c5edc7cd6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "siteID = \"b88e3083\"\n",
    "siteName = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}name\").text\n",
    "latitude = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}geography/{http://www.streit.cc/uddf/3.2/}latitude\").text\n",
    "latitude = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}geography/{http://www.streit.cc/uddf/3.2/}longitude\").text\n",
    "print(siteName + \" is at \" + latitude + \", \" + longitude)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd28344f-7b81-4bb1-b908-b646e9a6a694",
   "metadata": {},
   "source": [
    "Get dive site name for all dives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3ae725-ae4c-4377-a546-9206863c3ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for dive in root.iter('{http://www.streit.cc/uddf/3.2/}dive'):\n",
    "    diveID = dive.get('id')\n",
    "    siteID = dive.find('{http://www.streit.cc/uddf/3.2/}informationbeforedive').find('{http://www.streit.cc/uddf/3.2/}link').get('ref')\n",
    "    if siteID is not \"\":\n",
    "        siteName = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}name\").text\n",
    "    else:\n",
    "        siteName = \"no logged location\"\n",
    "    print('Dive ' + diveID + ' (' + siteName + \")\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7e1157-b628-460b-ac66-ef98bf70e9fd",
   "metadata": {},
   "source": [
    "Output details of all dive sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a6227f-1707-4d6b-89f3-9c0a80406be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for site in root.iter('{http://www.streit.cc/uddf/3.2/}site'):\n",
    "    print(site.get('id'))\n",
    "    for name in site.findall('{http://www.streit.cc/uddf/3.2/}name'):\n",
    "        print(name.text)\n",
    "    for geography in site.findall('{http://www.streit.cc/uddf/3.2/}geography'):\n",
    "        latitude = geography.find('{http://www.streit.cc/uddf/3.2/}latitude').text\n",
    "        longitude = geography.find('{http://www.streit.cc/uddf/3.2/}longitude').text\n",
    "        print(latitude + ', ' + longitude)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7bd1f9f-06a1-4e66-8c00-3c089f89ee6a",
   "metadata": {},
   "source": [
    "Match time stamp to a particular dive and dive site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4d752a-9ab0-49fa-852c-e2cf77ee2a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = datetime.fromisoformat(\"2022-02-05 10:24:04\")\n",
    "\n",
    "print(\"Target is \" + target.isoformat())\n",
    "print()\n",
    "for dive in root.iter('{http://www.streit.cc/uddf/3.2/}dive'):\n",
    "    diveID = dive.get('id')\n",
    "    diveStart = datetime.fromisoformat(dive.find('{http://www.streit.cc/uddf/3.2/}informationbeforedive').find('{http://www.streit.cc/uddf/3.2/}datetime').text)\n",
    "    diveDuration = dive.find('{http://www.streit.cc/uddf/3.2/}informationafterdive').find('{http://www.streit.cc/uddf/3.2/}diveduration').text\n",
    "    diveTimeDeltaDuration = timedelta(seconds=int(diveDuration))\n",
    "    diveEnd = diveStart + diveTimeDeltaDuration\n",
    "    print('target: ' + target.isoformat() + ' start: ' + diveStart.isoformat() + ' duration: ' + str(float(diveDuration)/60) + ' end: ' + diveEnd.isoformat())\n",
    "    if (diveStart < target and target < diveEnd):\n",
    "        print(\"Matching dive ID: \" + diveID)\n",
    "        siteID = dive.find('{http://www.streit.cc/uddf/3.2/}informationbeforedive').find('{http://www.streit.cc/uddf/3.2/}link').get('ref')\n",
    "        print(\"Site ID is: \" + siteID)\n",
    "        siteName = siteName = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}name\").text\n",
    "        print(\"Site name is: \" + siteName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e55d2c-4043-46c2-bcd5-7137d07e1055",
   "metadata": {},
   "outputs": [],
   "source": [
    "photoDateTime = datetime.fromisoformat(\"2022-02-05 10:00:19\")\n",
    "diveID = \"idm564921344\"\n",
    "previousDepth = 0.0\n",
    "\n",
    "siteID = dive.find('{http://www.streit.cc/uddf/3.2/}informationbeforedive/{http://www.streit.cc/uddf/3.2/}link').get('ref')\n",
    "if siteID != \"\":\n",
    "    siteName = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}name\").text\n",
    "    latitude = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}geography/{http://www.streit.cc/uddf/3.2/}latitude\").text\n",
    "    longitude = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}geography/{http://www.streit.cc/uddf/3.2/}longitude\").text\n",
    "dive = root.find(\"./{http://www.streit.cc/uddf/3.2/}profiledata/*[@id='\" + diveID+ \"']/{http://www.streit.cc/uddf/3.2/}dive\")\n",
    "diveStart = datetime.fromisoformat(root.find(\"./{http://www.streit.cc/uddf/3.2/}profiledata/*[@id='\" + diveID+ \"']/{http://www.streit.cc/uddf/3.2/}dive/{http://www.streit.cc/uddf/3.2/}informationbeforedive/{http://www.streit.cc/uddf/3.2/}datetime\").text)\n",
    "samples = root.find(\"./{http://www.streit.cc/uddf/3.2/}profiledata/*[@id='\" + diveID+ \"']/{http://www.streit.cc/uddf/3.2/}dive/{http://www.streit.cc/uddf/3.2/}samples\")\n",
    "previousTime = diveStart\n",
    "for waypoint in samples:\n",
    "    interval = diveStart + timedelta(seconds=int(waypoint.find(\"{http://www.streit.cc/uddf/3.2/}divetime\").text))\n",
    "    depth = float(waypoint.find(\"{http://www.streit.cc/uddf/3.2/}depth\").text)\n",
    "    if ( waypoint.find(\"{http://www.streit.cc/uddf/3.2/}temperature\") != None ):\n",
    "        temperature = round(float(waypoint.find(\"{http://www.streit.cc/uddf/3.2/}temperature\").text) - 273)\n",
    "    if ( previousTime <= photoDateTime and photoDateTime <= interval ):\n",
    "        print(interval.isoformat() + \": from \" + str(previousDepth) + \" to \" + str(depth))\n",
    "        print(\"Now!\")\n",
    "        break\n",
    "    previousDepth = float(waypoint.find(\"{http://www.streit.cc/uddf/3.2/}depth\").text)\n",
    "    previousTime = interval\n",
    "print(\"Photo taken at depth of \" + str(previousDepth) + \"m and \" + str(depth) + \"m at (\" + str(latitude) + \", \" + str(longitude) + \")\")\n",
    "if temperature != None:\n",
    "    print(\"The temperature was \" + str(temperature) + \"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0c2fa2-43d5-4337-8df2-d5ed2f2abd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "species = \"Hypselodoris saintvincentia\"\n",
    "\n",
    "filename = species\n",
    "originalName = species\n",
    "if ( '.' in species):\n",
    "    identificationQualifier = species.split(' ', 1)[1]\n",
    "    scientificName = species.split(' ', 1)[0]\n",
    "else:\n",
    "    scientificName = species\n",
    "    identificationQualifier = \"\"\n",
    "\n",
    "data = json.loads(getWoRMSTaxonData(scientificName))\n",
    "\n",
    "taxonRank = data[\"rank\"]\n",
    "taxonRank = taxonRank.lower()\n",
    "rankNumber = taxonHierarchy.index(taxonRank)\n",
    "\n",
    "aphiaID = data[\"AphiaID\"]\n",
    "family = \"\"\n",
    "genus = \"\"\n",
    "specificEpithet = \"\"\n",
    "scientificName = data[\"scientificname\"]\n",
    "scientificNameAuthorship = data[\"authority\"]\n",
    "taxonRemarks = data[\"citation\"]\n",
    "\n",
    "if (rankNumber >= 4):\n",
    "    family = data[\"family\"]\n",
    "if (rankNumber >= 5):\n",
    "    genus = data[\"genus\"]\n",
    "if (rankNumber >= 6):\n",
    "    specificEpithet = scientificName.split(\" \")[1]\n",
    "\n",
    "checkName = (scientificName + \" \" + identificationQualifier).strip()\n",
    "\n",
    "if ( originalName != checkName):\n",
    "    print(\"It looks like \" + originalName + \" has been reclassified to \" + checkName + \"! I will copy the photo files, but you will need to update your original metadata yourself.\")\n",
    "    filename = checkName\n",
    "    directories = [ thumbsDir, fullDir ]\n",
    "    for directory in directories:\n",
    "        with os.scandir(directory) as it:\n",
    "            for entry in it:\n",
    "                if entry.name.startswith(originalName) and entry.is_file():\n",
    "                    print(directory + \"/\" + entry.name + \" will be copied to \" + directory + \"/\" + entry.name.replace(originalName, checkName))\n",
    "                    shutil.copy(directory + \"/\" + entry.name, directory + \"/\" + entry.name.replace(originalName, checkName))\n",
    "                    with open(directory + '/' + entry.name.replace(originalName, checkName), 'rb') as photo:\n",
    "                        thisPhotoIPTC = IPTCInfo(photo)\n",
    "                        thisPhotoObjectName = thisPhotoIPTC[\"object name\"].decode(\"UTF-8\")\n",
    "                        print(thisPhotoObjectName + \" will be changed to \" + thisPhotoObjectName.replace(originalName, checkName))\n",
    "                        thisPhotoIPTC[\"object name\"] = bytes(thisPhotoObjectName.replace(originalName, checkName), \"UTF-8\")\n",
    "                        thisPhotoIPTC.save()\n",
    "\n",
    "    \n",
    "\n",
    "eolID = getEOLID(taxon)\n",
    "hierarchy = [ scientificName, genus, family ]\n",
    "\n",
    "wikipediaURL = getAWikipediaURL(hierarchy)\n",
    "\n",
    "print(\"Filename: \" + filename)\n",
    "print(\"---\")\n",
    "print(\"scientificName: \" + scientificName)\n",
    "print(\"family: \" + family)\n",
    "print(\"genus: \" + genus)\n",
    "print(\"specificEpithet: \" + specificEpithet)\n",
    "print(\"taxonRank: \" + taxonRank)\n",
    "print(\"scientificNameAuthorship: \" + scientificNameAuthorship)\n",
    "print(\"identificationQualifier: \" + identificationQualifier)\n",
    "print(\"colours:\")\n",
    "print(\"aphiaID: \" + str(aphiaID))\n",
    "print(\"eolID: \" + str(eolID))\n",
    "print(\"wikipediaURL: \" + wikipediaURL)\n",
    "print(\"taxonRemarks: \" + taxonRemarks)\n",
    "print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace815b-9b1e-4698-a6d0-02149f64a8e5",
   "metadata": {},
   "source": [
    "Go through all the photos and create a metadata file for each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dcf4f4-4996-4c85-ad59-8e2efd96d7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with os.scandir(thumbsDir) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_file():\n",
    "            with open(thumbsDir + '/' + entry.name, 'rb') as photo:\n",
    "                fileWritten = False\n",
    "                filename = entry.name.rsplit('.', 1)[0]\n",
    "                scientificName = filename.split('-')[0].capitalize()\n",
    "                if ( '.' in scientificName):\n",
    "                    identificationQualifier = scientificName.split(' ', 1)[1]\n",
    "                    scientificName = scientificName.split(' ', 1)[0]\n",
    "                else:\n",
    "                    identificationQualifier = \"\"\n",
    "                thisPhoto = Image(photo)\n",
    "                occurrenceRemark = \" \"\n",
    "                try:\n",
    "                    occurrenceRemark = thisPhoto.get(\"image_description\")\n",
    "                    if (type(occurrenceRemark) != None):\n",
    "                        occurrenceRemark = occurrenceRemark.replace(\"\\n\",\" \")\n",
    "                except:\n",
    "                    print(\"Couldn't extract image description\")\n",
    "                photoDateTime = datetime.fromisoformat(thisPhoto.datetime_original.replace(\":\", \"-\", 2))\n",
    "                try:\n",
    "                    photographer = thisPhoto.artist\n",
    "                except:\n",
    "                    print(\"Couldn't extract photographer\")\n",
    "                # print(entry.name + ' was taken at: ' + photoDateTime.isoformat())\n",
    "                for dive in root.iter('{http://www.streit.cc/uddf/3.2/}dive'):\n",
    "                    diveID = dive.get('id')\n",
    "                    diveStart = datetime.fromisoformat(dive.find('{http://www.streit.cc/uddf/3.2/}informationbeforedive/{http://www.streit.cc/uddf/3.2/}datetime').text)\n",
    "                    diveDuration = dive.find('{http://www.streit.cc/uddf/3.2/}informationafterdive/{http://www.streit.cc/uddf/3.2/}diveduration').text\n",
    "                    diveTimeDeltaDuration = timedelta(seconds=int(diveDuration))\n",
    "                    diveEnd = diveStart + diveTimeDeltaDuration\n",
    "                    if (diveStart <= photoDateTime and photoDateTime <= diveEnd):\n",
    "                        siteID = dive.find('{http://www.streit.cc/uddf/3.2/}informationbeforedive/{http://www.streit.cc/uddf/3.2/}link').get('ref')\n",
    "                        if siteID != \"\":\n",
    "                            siteName = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}name\").text\n",
    "                            latitude = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}geography/{http://www.streit.cc/uddf/3.2/}latitude\").text\n",
    "                            longitude = root.find(\"./{http://www.streit.cc/uddf/3.2/}divesite/*[@id='\" + siteID+ \"']/{http://www.streit.cc/uddf/3.2/}geography/{http://www.streit.cc/uddf/3.2/}longitude\").text\n",
    "                            # print(\"Matching dive \" + diveID + \" found at \" + siteName + \" with coordinates \" + latitude + \", \" + longitude + \" starting at \" + diveStart.isoformat())\n",
    "                            previousDepth = 0.0\n",
    "                            samples = root.find(\"./{http://www.streit.cc/uddf/3.2/}profiledata/*[@id='\" + diveID+ \"']/{http://www.streit.cc/uddf/3.2/}dive/{http://www.streit.cc/uddf/3.2/}samples\")\n",
    "                            previousTime = diveStart\n",
    "                            for waypoint in samples:\n",
    "                                interval = diveStart + timedelta(seconds=int(waypoint.find(\"{http://www.streit.cc/uddf/3.2/}divetime\").text))\n",
    "                                depth = float(waypoint.find(\"{http://www.streit.cc/uddf/3.2/}depth\").text)\n",
    "                                if ( waypoint.find(\"{http://www.streit.cc/uddf/3.2/}temperature\") != None ):\n",
    "                                    temperature = round(float(waypoint.find(\"{http://www.streit.cc/uddf/3.2/}temperature\").text) - 273)\n",
    "                                if ( previousTime <= photoDateTime and photoDateTime <= interval ):\n",
    "                                    # print(\"Photo taken in interval between \" + previousTime.isoformat() + \" and \" + interval.isoformat())\n",
    "                                    break\n",
    "                                previousDepth = float(waypoint.find(\"{http://www.streit.cc/uddf/3.2/}depth\").text)\n",
    "                                previousTime = interval\n",
    "                            with open(observationDir + \"/\" + filename + \".md\", \"w\") as f:\n",
    "                                f.write(\"---\\n\")\n",
    "                                f.write(\"# Record-level terms\\n\")\n",
    "                                f.write(\"type: StillImage\\n\")\n",
    "                                f.write(\"basisOfRecord: HumanObservation\\n\")\n",
    "                                f.write(\"# Occurrence terms\\n\")\n",
    "                                f.write(\"recordedBy: \" + (photographer if photographer != None else \"\") + \"\\n\")\n",
    "                                f.write(\"recordedByID: \" + ORCID + \"\\n\")\n",
    "                                f.write('occurrenceRemarks: \"' + (occurrenceRemark if occurrenceRemark != None else \"\") + '\"\\n')\n",
    "                                f.write(\"# Event terms\\n\")\n",
    "                                f.write(\"eventDateTime: \" + photoDateTime.isoformat() + \"\\n\")\n",
    "                                f.write(\"year: \" + str(photoDateTime.year) + \"\\n\")\n",
    "                                f.write(\"month: \" + str(photoDateTime.month) + \"\\n\")\n",
    "                                f.write(\"day: \" + str(photoDateTime.day) + \"\\n\")\n",
    "                                f.write(\"# Location terms\\n\")\n",
    "                                f.write(\"locationRemarks: \" + str(siteName) + \"\\n\")\n",
    "                                f.write(\"minimumDepthInMeters: \" + str(previousDepth) + \"\\n\")\n",
    "                                f.write(\"maximumDepthInMeters: \" + str(depth) + \"\\n\")\n",
    "                                f.write(\"decimalLatitude: \" + str(latitude) + \"\\n\")\n",
    "                                f.write(\"decimalLatitude: \" + str(longitude) + \"\\n\")\n",
    "                                f.write(\"temperature: \" + str(temperature) + \"\\n\")\n",
    "                                f.write(\"# Identification terms\\n\")\n",
    "                                f.write(\"identifiedBy: \\n\")\n",
    "                                f.write(\"identifiedByID: \\n\")\n",
    "                                f.write(\"# Taxon terms\\n\")\n",
    "                                f.write(\"scientificName: \" + scientificName + \"\\n\")\n",
    "                                f.write(\"identificationQualifier: \" + identificationQualifier + \"\\n\")\n",
    "                                f.write(\"taxonRank: \" + (\"species\" if identificationQualifier == None else \"genus\") + \"\\n\")\n",
    "                                f.write(\"---\\n\")\n",
    "                                print(observationDir + \"/\" + filename + \".md\" + \" written\")\n",
    "                                fileWritten = True\n",
    "                        else:\n",
    "                            print(\"Matching dive \" + diveID + \" has no logged location! Cannot write data file.\")\n",
    "            if ( fileWritten == False ):\n",
    "                    print(\"No file was written for \" + filename + \"! Perhaps the photo didn't match a logged dive?\")\n",
    "                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f82de6d-fecd-401c-8629-0eb5643a9e04",
   "metadata": {},
   "source": [
    "Create species entries for all the obervations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5014c6ec-8cfa-42d6-9b25-8aedb625927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "speciesList = list()\n",
    "with os.scandir(observationDir) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_file():\n",
    "            with open(observationDir + '/' + entry.name, 'rb') as observation:\n",
    "                filename = entry.name.rsplit('.', 1)[0]\n",
    "                scientificName = filename.split('-')[0].capitalize()\n",
    "                if ( scientificName not in speciesList ):\n",
    "                    speciesList.append(scientificName)\n",
    "speciesList.sort()\n",
    "for species in speciesList:\n",
    "    filename = species\n",
    "    originalName = species\n",
    "    if ( '.' in species):\n",
    "        identificationQualifier = species.split(' ', 1)[1]\n",
    "        scientificName = species.split(' ', 1)[0]\n",
    "    else:\n",
    "        scientificName = species\n",
    "        identificationQualifier = \"\"\n",
    "        \n",
    "    data = json.loads(getWoRMSTaxonData(scientificName))\n",
    "    \n",
    "    taxonRank = data[\"rank\"]\n",
    "    taxonRank = taxonRank.lower()\n",
    "    rankNumber = taxonHierarchy.index(taxonRank)\n",
    "\n",
    "    aphiaID = data[\"AphiaID\"]\n",
    "    family = \"\"\n",
    "    genus = \"\"\n",
    "    specificEpithet = \"\"\n",
    "    scientificName = data[\"scientificname\"]\n",
    "    scientificNameAuthorship = data[\"authority\"]\n",
    "    taxonRemarks = data[\"citation\"]\n",
    "\n",
    "    if (rankNumber >= 4):\n",
    "        family = data[\"family\"]\n",
    "    if (rankNumber >= 5):\n",
    "        genus = data[\"genus\"]\n",
    "    if (rankNumber >= 6):\n",
    "        specificEpithet = scientificName.split(\" \")[1]\n",
    "\n",
    "    checkName = (scientificName + \" \" + identificationQualifier).strip()\n",
    "    \n",
    "    if ( originalName != checkName):\n",
    "        print(\"It looks like \" + originalName + \" has been reclassified to \" + checkName + \"!\")\n",
    "\n",
    "    eolID = getEOLID(scientificName)\n",
    "    hierarchy = [ scientificName, genus, family ]\n",
    "\n",
    "    wikipediaURL = getAWikipediaURL(hierarchy)\n",
    "        \n",
    "    with open(speciesDir + \"/\" + filename + \".md\", \"w\") as f:\n",
    "        f.write(\"---\\n\")\n",
    "        f.write(\"scientificName: \" + scientificName + \"\\n\")\n",
    "        f.write(\"family: \" + family + \"\\n\")\n",
    "        f.write(\"genus: \" + genus + \"\\n\")\n",
    "        f.write(\"specificEpithet: \" + specificEpithet + \"\\n\")\n",
    "        f.write(\"taxonRank: \" + taxonRank + \"\\n\")\n",
    "        f.write(\"scientificNameAuthorship: \" + scientificNameAuthorship + \"\\n\")\n",
    "        f.write(\"identificationQualifier: \" + identificationQualifier + \"\\n\")\n",
    "        f.write(\"colours:\\n\")\n",
    "        f.write(\"aphiaID: \" + str(aphiaID) + \"\\n\")\n",
    "        f.write(\"eolID: \" + str(eolID) + \"\\n\")\n",
    "        f.write('wikipediaURL: \"' + wikipediaURL + '\"\\n')\n",
    "        f.write('taxonRemarks: \"' + taxonRemarks + '\"\\n')\n",
    "        f.write(\"---\\n\")\n",
    "    print(speciesDir + \"/\" + filename + \".md\" + \" written\")\n",
    "\n",
    "    time.sleep(0.1)\n",
    "    \n",
    "print ( str(len(speciesList)) + \" total species\" )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792aab2a-9df6-4d37-9e90-2b2f40b8f90c",
   "metadata": {},
   "source": [
    "Sanitise file names and zip outputs for easy downloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224e2701-c391-4077-b549-a27ee1584353",
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [ fullDir, thumbsDir, observationDir, speciesDir ]\n",
    "\n",
    "# Empty target directories first\n",
    "\n",
    "for directory in directories:\n",
    "    with os.scandir(outputDir + \"/\" + directory) as it:\n",
    "        for entry in it:\n",
    "            if not entry.name.startswith('.') and entry.is_file():\n",
    "                os.remove(entry)\n",
    "                \n",
    "with os.scandir(outputDir) as it:\n",
    "    for entry in it:\n",
    "        if not entry.name.startswith('.') and entry.is_file():\n",
    "            os.remove(entry)\n",
    "\n",
    "for directory in directories:\n",
    "    with os.scandir(directory) as it:\n",
    "        for entry in it:\n",
    "            if not entry.name.startswith('.') and entry.is_file():\n",
    "                originalName = entry.name\n",
    "                fileName = originalName.rsplit(\".\",1)[0]\n",
    "                extension = originalName.rsplit(\".\",1)[1]\n",
    "                newName = \".\".join([fileName.lower().replace(\".\",\"\"), extension])\n",
    "                print(directory + \"/\" + originalName + \" -> \" + outputDir + \"/\" + directory + \"/\" + newName)\n",
    "                shutil.copy(directory + \"/\" + originalName, outputDir + \"/\" + directory + \"/\" + newName)\n",
    "\n",
    "with zipfile.ZipFile(outputDir + '/output.zip', 'w') as outputzip:\n",
    "    for directory in directories:\n",
    "        with os.scandir(outputDir + \"/\" + directory) as it:\n",
    "            for entry in it:\n",
    "                if not entry.name.startswith('.') and entry.is_file():\n",
    "                    outputzip.write(outputDir + \"/\" + directory + \"/\" + entry.name )              \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
